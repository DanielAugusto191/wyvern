-------------------------------------------------------------------------------

(1) "A Theory of Lazy Imperative Timing"
Eric C. R. Hehner (U of Toronto)
Refine'18 (Workshop)
URL: https://arxiv.org/abs/1810.09610

This paper is really only tangentially related, in that it deals with the
intersection of lazy evaluation and imperative languages. The authors propose
a theory for precisely timing the execution of programs written in imperative
languages if parts of them were to be evaluated lazily. There's no
implementation, it seems to be just a thought experiment.

-------------------------------------------------------------------------------

(2) "Lazy Imperative Programming"
John Launchbury (Glasgow U)
ACM Workshop on State in Programming Languages 1993
URL: https://www.researchgate.net/publication/2248360_Lazy_Imperative_Programming

The author of this proposes a way to model imperative state within Haskell, which
is a purely lazily-evaluated language. At the time, the only way to implement
mutable state in Haskell was through monads (such as the IO monad), which had
the unfortunate side-effect of losing the benefits of lazy evaluation, as those
monads were always eagerly evaluated. The author proposes a language construct
"Seq", which allows modeling sequential/imperative state computations, but
allowing them to be lazily evaluated when possible.

-------------------------------------------------------------------------------

(3) "Lazy Code Motion"
Jens Knoop (Vienna UT), Olier Ruthing, Bernhard Steffen (TU Dortmund)
ACM PLDI'92
URL: https://dl.acm.org/doi/abs/10.1145/143095.143136

The classic paper on Lazy Code Motion/Partial Redundancy Elimination. The
obvious relation to our work is that it is "lazy", in the sense that it removes
redundant expression computations, replacing them by as few computations as
possible.

-------------------------------------------------------------------------------

(4) "Interprocedural partial redundancy elimination and its application to
distributed memory compilation" (why so long?)
Gargan Agrawal, Joel Haskin Saltz, Raja Das (University of Maryland, College)
URL: https://dl.acm.org/doi/abs/10.1145/223428.207157

This is probably the most closely related paper we have found so far. The
authors propose a technique that identifies pure functions which are called
in contexts where their arguments are never modified, and hoisting those calls
out to the callee function. Their optimization however focuses solely on the
scope of function calls, and bails out whenever a function's argument is
modified in a program path. Our technique therefore could actually be
complementary with theirs: after being transformed by our optimization, a
program is more likely to contain pure function calls that can be hoisted
out interprocedurally.
